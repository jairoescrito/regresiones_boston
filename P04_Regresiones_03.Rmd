---
title: "COMPARACIÓN DE RESULTADOS DE MODELOS DE REGRESIÓN"
author: "Jairo Sánchez"
date: "2023-11-22"
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse) # Librería para ordenamiento de arreglos de datos, incluye ggplot2
library(Metrics) # Librería para calculo de métricas de evaluación valores reales vs valores predichos
library(plotly) # gráficos interactivos
library(knitr) # Libería para crear tablas en archivos Knit html# gráficos interactivos
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Carga de datos actualizados en la parte anterior
Train <- as.data.frame(read_csv("Train.csv", col_types = cols(chas = col_factor()
                                                , rad = col_factor())))
Test <- as.data.frame(read_csv("Test.csv", col_types = cols(chas = col_factor(), 
                                              rad = col_factor())))
RMSE_3 <- read_csv("RMSE_2.csv")
```

```{=html}
<center>
<h1>Parte 4: Regresiones lineales múltiples con interacciones</h1>
</center>
```
## Introducción

En la [parte 3](https://rpubs.com/jairoescrito/Regresiones_3) de este documento se plantearon regresiones múltiples en función de 12, 10 y 2 variables independientes, en los modelos no se incluyeron, como componentes de la ecuación, el producto entre al menos dos variables y la estimación respectiva del coeficiente. En este capítulo se presentarán algunos modelos con ésta interacción, la decisión de las variables a incluir se tomará conforme a los resultados de los coeficientes de correlación expuestos en la [parte 1](https://rpubs.com/jairoescrito/Regresiones_1) del documento.

### 4.1. Regresión lineal múltiple con interacción de dos variables independientes

El primer modelo propuesto involucra las dos variables de "mayor relevancia" en cuanto a la influencia sobre la variable independiente: *rm* y *lstat.*

```{r echo=FALSE, message=FALSE, warning=FALSE}
lm_mi2vars <- lm(data = Train, medv~lstat*rm)
summary(lm_mi2vars)
```

El modelo concluye con un porcentaje de 73% en el nivel de explicación del comportamiento de la variable medv, un valor similar a los modelos de regresión lineal múltiple de 10 y 12 variables. En cuanto al error estándar residual se observa en 4.83 miles de dólares. Es de anotar que los resultados iniciales del presente modelo lo hacen más atractivos que los dos primeros expuestos en la [parte 3](https://rpubs.com/jairoescrito/Regresiones_3) de este documento, en razón a que es un modelo más sencillo dada la cantidad de variables independientes involucradas. Todos los parámetros estimados se consideran significativos (no se concluye que alguno sea cero), así, el modelo se podría escribir así:

$$
medv = 2.15385*lstat + 9.41133*rm - 0.48468*lstat*rm - 26.82720
$$

Los resultados de las predicciones para el subconjuto de datos Test frente a los datos reales se observan en la siguiente gráfica

```{r echo=FALSE, message=FALSE, warning=FALSE}
p_mi2vars <- predict(lm_mi2vars,Test)
data_reg <- cbind(Test[,c(6,12,13)],round(p_mi2vars,1))
nombres <- names(data_reg[-4])
nombres <- c(nombres,"medv_predict_mi2vars")
names(data_reg) <-nombres 
rm(nombres)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_mi2vars <- plot_ly(data = data_reg, x = ~lstat, y = ~rm) # Definición básica de la gráfica
fig_mi2vars <- fig_mi2vars %>% add_trace(z = ~medv, # Agregar trazado de datos reales
                                           mode = 'markers',
                                           size = 0,
                                           name = 'medv_real')
fig_mi2vars<- fig_mi2vars %>% add_trace(z = ~medv_predict_mi2vars, # Agregar trazado de datos predicción
                                           mode = 'markers',
                                           size = 0,
                                           name = 'medv_predicción_mi2vars')
fig_mi2vars <- fig_mi2vars %>% layout(title = list(text ='Regresión lineal múltiple interacción 2 variables',
                                                     y = 0.97, x = 0.1),
                                        legend = list(orientation = 'h', 
                                                      x = 0.28, y = -0.2))
fig_mi2vars # Mostrar gráfico
```

Los resultados de este modelo de regresión presentan un ajuste mejorado para los valores extremos (valores altos y bajos que toma *medv*), los cuales son los que han aumentado el valor del error de los modelos hasta el momento vistos. Pese a que se siguen observando puntos de *medv* real alejados de su par en la predicción, la distancia observada visualmente parece menores en comparación con otros modelos ya estudiados.

### 4.2. Regresión lineal múltiple con interacción de tres variables independientes

Para el modelo con 3 variables se agrega, a las dos anteriores, *ptratio,* la cual es la tercera con valor más alto en el coeficiente de correlación con la variable *medv.*

```{r echo=FALSE, message=FALSE, warning=FALSE}
lm_mi3vars <- lm(data = Train, medv~lstat*rm*ptratio)
summary(lm_mi3vars)
```

Los resultados para este modelo muestran un aumento de 6 puntos porcentuales en el valor del R² ajustado en relación al modelo expuesto en el numeral 4.1, esto es, se alcanza un valor de alrededor del 79% de explicación del comportamiento de la variable *medv*, sumado a lo anterior el error estándar residual se encuentra en 4.31 lo cual representa una reducción de 500 dólares con respecto al modelo anterior. La ecuación del modelo de regresión lineal múltiple con interacción de 3 variables queda de la siguiente manera:

$$
medv = 8.72447*lstat + 41.84550*rm + 12.28417*ptratio - 1.25939*lstat*rm - 0.38473*lstat*ptratio 
$$

$$
- 1.85583*rm*ptratio + 0.04804*lstat*rm*ptratio
$$

Los parámetros del estimados del modelo se consideran significativos, es decir, de acuerdo a los resultados del p-valor ninguno se considera igual a cero.

Usando la anterior ecuación, se calculan las predicciones de *medv* y se comparan con los valores reales de la misma variable, esto resultados se presentan en la siguiente gráfica

```{r echo=FALSE, message=FALSE, warning=FALSE}
p_mi3vars <- predict(lm_mi3vars,Test)
data_reg <- cbind(data_reg,Test[,11],round(p_mi3vars,1))
nombres <- names(data_reg[-c(5,6)])
nombres <- c(nombres,"ptratio","medv_predict_mi3vars")
names(data_reg) <-nombres 
rm(nombres)
data_reg<- data_reg %>% relocate(ptratio,.after = lstat)
data_reg<- arrange(data_reg,medv)
data_reg$Obs <- rep(1:nrow(Test))
data_reg<- data_reg %>% relocate(Obs,.before = rm)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_mi3vars <- plot_ly(data = data_reg, x = ~Obs) # Definición básica de la gráfica
fig_mi3vars <- fig_mi3vars %>% add_trace(y = ~medv, # Agregar trazado de datos reales
                                           mode = 'lines+markers',
                                           name = 'medv_real',
                                           type="scatter")
fig_mi3vars <- fig_mi3vars %>% add_trace(y = ~medv_predict_mi3vars, # Agregar trazado de datos predicción
                                           mode = 'lines+markers',
                                           name = 'medv_predicción_mi3vars',
                                           type="scatter")
fig_mi3vars <- fig_mi3vars %>% layout(title = list(text ='Regresión lineal múltiple interacción 3 variables',
                                                     y = 0.97, x = 0.1),
                                        xaxis = list(title = 'Observación'),
                                        yaxis = list(title = 'medv'))
# Imprimir gráfica
fig_mi3vars
```

Los resultados del error estándar residual ya mencionados se reflejan en el gráfico anterior. La predicción se ajusta de mejor manera a los datos extremos (máximos y mínimos). Si bien existen predicciones bastante alejadas de algunos datos reales, esta es la primera regresión que presenta predicciones con valores por encima de los valores más altos de la variable de respuesta (50 mil dólares), a diferencia de los otros modelos de regresión anteriores que presentaban resultados por debajo de los valores reales en el rango de 37 a 50 mil dólares, aproximadamente.

### 4.3. Regresión lineal múltiple con interacción de cuatro variables independientes

```{r echo=FALSE, message=FALSE, warning=FALSE}
lm_mi4vars <- lm(data = Train, medv~lstat*rm*ptratio*chas)
summary(lm_mi4vars)
```

En este modelo se incluye una de las variables categóricas: *chas*. De los modelos presentados es el que entrega el mayor valor de R cuadrado ajustado: 0.81, esto es, el modelo explica en un 81% el comportamiento de la variable *medv*. Adicionalmente, el error estándar residual es de 4.07 el dato más bajo observado hasta el momento. En cuanto al modelo definitivo, los coeficientes estimados no presentan significancia para todas las variables o interacción entre ellas. Si se plantea un nivel de confianza del 95%, 11 de los 15 coeficientes del modelo podrían considerarse igual a cero: lstat \* chas*,* rm \* chas, lstat \* rm \* chas, lstat \* ptratio \* chas, lstat \* rm \* ptratio \* chas, esto es, se excluyen del modelo.

Con base en lo anterior se plantea nuevamente el modelo excluyendo las interacciones mencionadas (cabe resaltar que en este caso el modelo deja de ser de 4 variables y se convierte en un modelo de 3 variables)

```{r echo=FALSE, message=FALSE, warning=FALSE}
lm_mi4vars <- lm(data = Train, medv~lstat+rm+ptratio+chas+
                   rm*ptratio+
                   ptratio*chas+
                   rm*ptratio*chas)
summary(lm_mi4vars)
```

La comparación de las predicciones frente a los valores reales de la variable independiente medv se presentan en la siguiente gráfica

```{r echo=FALSE, message=FALSE, warning=FALSE}
p_mi4vars <- predict(lm_mi4vars,Test)
data_reg <- rownames_to_column(data_reg,"Ind")
data_reg$Ind <- as.numeric(data_reg$Ind)
data_reg<- arrange(data_reg,Ind)
data_reg <- cbind(data_reg,Test[,4],round(p_mi4vars,1))
nombres <- names(data_reg[-c(9,10)])
nombres <- c(nombres,"chas","medv_predict_mi4vars")
names(data_reg) <-nombres 
rm(nombres)
data_reg<- data_reg %>% relocate(chas,.after = ptratio)
data_reg<- arrange(data_reg,medv)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_mi4vars <- plot_ly(data = data_reg, x = ~Obs) # Definición básica de la gráfica
fig_mi4vars <- fig_mi4vars %>% add_trace(y = ~medv, # Agregar trazado de datos reales
                                           mode = 'lines+markers',
                                           name = 'medv_real',
                                           type="scatter")
fig_mi4vars <- fig_mi4vars %>% add_trace(y = ~medv_predict_mi4vars, # Agregar trazado de datos predicción
                                           mode = 'lines+markers',
                                           name = 'medv_predicción_mi4vars',
                                           type="scatter",
                                           line=list(color='indianred'),
                                           marker=list(color='indianred'))
fig_mi4vars <- fig_mi4vars %>% layout(title = list(text ='Regresión lineal múltiple interacción 4 variables',
                                                     y = 0.97, x = 0.1),
                                        xaxis = list(title = 'Observación'),
                                        yaxis = list(title = 'medv'))
# Imprimir gráfica
fig_mi4vars
```

Los resultados observados son muy similares a los del modelo con 3 variables, es posible que los resultados de la predicción estén ligeramente más cerca a los valores reales, sin embargo, esta afirmación solo se puede validar con el cálculo del RMSE

### 4.4 Comparación de regresiones lineales múltiples con interacciones

En la siguiente gráfica se comparan los resultados de predicción de los 3 modelos con los valores reales de la variable *medv.*

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_mi <- plot_ly(data = data_reg, x = ~Obs) # Definición básica de la gráfica
fig_mi <- fig_mi %>% add_trace(y = ~medv, # Agregar trazado de datos reales
                                           mode = 'lines+markers',
                                           name = 'medv_real',
                                           type="scatter")
fig_mi <- fig_mi %>% add_trace(y = ~medv_predict_mi2vars, # Agregar trazado de datos predicción
                                           mode = 'lines+markers',
                                           name = '2 vars',
                                           type="scatter")
fig_mi <- fig_mi %>% add_trace(y = ~medv_predict_mi3vars, # Agregar trazado de datos predicción
                                           mode = 'lines+markers',
                                           name = '3 vars',
                                           type="scatter")
fig_mi <- fig_mi %>% add_trace(y = ~medv_predict_mi4vars, # Agregar trazado de datos predicción
                                           mode = 'lines+markers',
                                           name = '4 vars',
                                           type="scatter")
fig_mi <- fig_mi %>% layout(title = list(text ='Modelos de regresión lineal múltiple con interacción',
                                                     y = 0.97, x = 0.1),
                                        xaxis = list(title = 'Observación'),
                                        yaxis = list(title = 'medv'))
# Imprimir gráfica
fig_mi
```

Los modelos presentan una tendencia similar, siendo el de dos variables el que parece alejarse más de los valores reales mientras que el de 4 variables presenta rangos en los que las predicciones se ubican más cercanas a los valores reales. No obstante, los tres modelos tienden a mejorar la situación con los valores reales extremos, en especial para los valores de *medv* real por encima de 37; las predicciones se ubican mucho más cerca en comparación de los modelos planteados en las secciones anteriores de este documento.

#### **4.4.1 Cálculo del RMSE**

Se calculan tres nuevos valores de RMSE, esta vez para los modelos de regresión múltiple con interacción expuestos en esta sección, igual, se comparán con los resultados RMSE de los modelos ya estudiados.

El RMSE del modelo con interacción de 2 variables se calcula en 4.18, el del modelo con interacción de 3 variables se observa en 3.99 mientras que el de 4 variables alcanza un valor de 3.89.

A medida que en los modelos se aumenta el número de variables, el RMSE calculado con el conjunto de datos Test disminuye. Al comparar estos nuevos resultados con los modelos planteados en las secciones anteriores se observa ya una reducción singificativa en el error (ya alcanza los 2300 dólares). Un menor valor de RMSE implica un modelo más complejo.

```{r echo=FALSE, message=FALSE, warning=FALSE}
e_mi2vars <- round(rmse(data_reg$medv,data_reg$medv_predict_mi2vars),2)
e_mi3vars <- round(rmse(data_reg$medv,data_reg$medv_predict_mi3vars),2)
e_mi4vars <- round(rmse(data_reg$medv,data_reg$medv_predict_mi4vars),2)
RMSE_3 <- as.data.frame(rbind(RMSE_3,e_mi2vars,e_mi3vars,e_mi4vars))
RMSE_3[,1] <- c("01_RLS_lstat","02_RLS_rm","03_RLM_12vars"
                ,"04_RLM_2vars","05_RLMI_2vars", "06_RLMI_3vars", "07_RLMI_4vars")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fig_rmse <- plot_ly(RMSE_3, x = ~Modelo, y = ~RMSE, 
                type = "scatter", 
                mode = "markers+lines")
fig_rmse <- fig_rmse %>% layout(xaxis = list(tickangle=-45))
fig_rmse
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
write_csv(RMSE_3,"RMSE_3.csv")
```
