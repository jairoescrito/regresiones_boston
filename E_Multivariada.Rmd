---
title: "COMPARACIÓN DE RESULTADOS DE MODELOS DE REGRESIÓN"
author: "Jairo Sánchez"
date: "2023-11-17"
output: html_document
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ISLR2) # Librería con datasets y herramientas del libro An Introduction
# to Statistical Learning with R
library(knitr) # Libería para crear tablas en archivos Knit html
library(tidyverse) # Librería para ordenamiento de arreglos de datos, incluye ggplot2
library(cowplot) # Librería para configurar los lienzos de gráficas, para graficar varias en uno solo
library(GGally) # Librería que contiene la función para elaborar pares de 
# gráficos de dispersión usando las herramientas visuales de ggplot2
library(corrplot) # Librería para la exploración visual de la matriz de 
# correlaciones de un conjunto de datos
```

```{=html}
<center>
<h1>Parte 1: Estadística Multivariada</h1>
</center>
```
## Introducción

Este documento presenta un comparativo de los resultados de predicción de datos entre diferentes modelos de regresión numérica. Para este ejercicio se usa el conjunto de datos Boston disponible e la librería ISLR2. Este es un conjunto de datos comúnmente usado para la ilustración de aplicación de modelos de regresión. *James, G., Witten, D., Hastie, T., & Tibshirani* en su libro An Introduction to Statistical Learning presentan el desarrollo de una regresión lineal simple y una regresión lineal múltiple, entre otras, usando el dataset mencionado; *R.Chollet, F., & Allaire, J. J* en su libro *Deep Learning with R* explican la aplicación de modelos de Redes Neuronales Artificiales en regresiones numéricas haciendo uso de este conjunto de datos, solo por mencionar algunos.

### 1.1. Exploración inicial del conjunto de datos

El dataset Boston utilizado en este desarrollo se carga desde la librería ISRL2 (librería que hace parte del libro An Introduction to Statistical Learning with Applications in R Second Edition) presenta 13 variables con información promedio de las viviendas en la ciudad de Boston (otras versiones de este conjunto de datos contemplan una variable más) . En este ejercicio se contempla una única variable dependiente que hace referencia al valor medio de las viviendas (en miles de dólares) que está en función de las características promedio de las viviendas y del vecindario:

1)  CRIM - razón de crimen per cápita del vecindario
2)  ZN - proporción de terreno residencial zonificado (lotes por encima de 25 mil pies cuadrados)
3)  INDUS - proporción de negocios no minoristas (grandes empresas o industrias)
4)  CHAS - Proximidad al río Charles (1 si limita con el río; 0 para los demás casos)
5)  NOX - concentración de óxido nítrico (partes por 10 millones)
6)  RM - número medio de habitaciones por vivienda
7)  AGE - proporción de unidades ocupadas por sus propietarios (construidas antes de 1940)
8)  DIS - distancia ponderada a los cinco centros de empleo de la ciudad
9)  RAD - índice de accesibilidad a las autopistas
10) TAX - tasa de impuesto de la propiedad por cada 10mil dólares
11) PTRATIO - razón maestros-alumnos en el vecindario
12) LSTAT - porcentaje de población de bajo estrato en el vecindario
13) MEDV - valor medio de las viviendas ocupadas en el vecindario (miles de dólares)

El conjunto de datos dispone de 506 observaciones.

```{r echo=FALSE, message=FALSE, warning=FALSE}
Data <- Boston
Data$rad <- as.factor(Data$rad) # convertir a categórica
Data$chas <- as.factor(Data$chas) # convertir a categórica
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr :: kable(head(Data),
               caption = "Dataset Boston", "pipe")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
  knitr :: kable(summary(Data),
               caption = "Resumen de las variables", "pipe")
```

La variable dependiente "medv" toma valores entre 5 y 50, es decir, el valor de las casas en Boston oscila entre 5mil y 50mil dólares (es de anotar que la información del dataset se publicó por primera vez en el año 1978: <https://lib.stat.cmu.edu/datasets/boston>); el valor promedio es de 22.530 dólares y el 75% de las observaciones registran un valor medio menor o igual a 25mil dólares.

#### 1.1.1. Análisis gráfico de la dispersión de los datos

En el caso del valor medio de las propiedades, la información descrita anteriormente se podría visualizar gráficamente así:

```{r echo=FALSE, message=FALSE, warning=FALSE}
bp_medv <- ggplot(data = Data, aes(x = "", y=medv)) +
  geom_boxplot() + 
  stat_summary(fun.data = "mean_cl_normal", 
              aes(shape="Media"), 
              colour = "blue",
              geom="point",
              size = 2) +
  labs(title = "Boxplot para medv", 
       subtitle = "Dispersión del valor medio de las propiedades") +
  theme(axis.title.x = element_blank(),
        legend.title = element_blank())
bp_medv
```

En cuanto a las variables independientes, la dispersión de los datos se presenta en la siguiente gráfica (previo se modifica estandariza la escala de los datos de tal manera que pueda compararse su dispersión)

```{r echo=FALSE, message=FALSE, warning=FALSE}
SData <- data.frame(scale(Data[,-c(4,9,13)]))
SData <- as.data.frame(pivot_longer(SData, cols = 1:10, 
                                    names_to = "Variables", 
                                    values_to = "Valores"))
bp_all <- ggplot(data = SData, aes(x = Variables, y = Valores)) + 
  geom_boxplot() +
  stat_summary(fun.data = "mean_cl_normal", 
              aes(shape="Media"), 
              colour = "red",
              geom="point",
              size = 2) +
  labs(title = "Boxplot para variables independientes", 
       subtitle = "Dispersión de los valores") +
  theme(legend.title = element_blank())
bp_all
```

Para las variables numéricas (se excluye "chas" y "rad") se evidencia variabilidad considerable para "crim" y en menor proporción para "rm" y "zn". La dispersión total (que se presentara más adelante con el calculo de varianza total) supone un reto para los modelos planteados en cuando a su precisión y capacidad de predicción.

Con respecto a las variables categóricas incluidas en el conjunto de datos, mediante un gráfico de barras se presentan los valores más frecuentes. En el caso de la variable "chas", la mayoría de las observaciones se relacionan a vecindarios que no están en la rivera del río Charles mientras que para el índice de accesibilidad a las autopistas (rad) los valores 4, 5 y 24 son los más frecuentes.

```{r echo=FALSE, message=FALSE, warning=FALSE}
bar_chas<-ggplot(Data,aes(x=chas)) +
  geom_bar(fill = "#009E73") +
  labs(title = "Chas", 
       subtitle = "Conteo de observaciones") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())
bar_rad<-ggplot(Data,aes(x=rad)) +
  geom_bar(fill = "#56B4E9") +
  labs(title = "Rad",
       subtitle = "Conteo de observaciones") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())
# Visualización de gráficos
plot_grid(bar_chas, bar_rad)
```

La relación existente entre las variables independientes y la variable de respuesta se puede observar mediante gráfico de pares, el cual presenta datos relevantes para determinar aquellas variables que presentan una mayor influencia en la variable de respuesta y así tener una idea inicial respecto a cuales podrían explicar su variabilidad. En este gráfico se excluyen las variables categóricas.

```{r echo=FALSE, message=FALSE, warning=FALSE}
gpairs <- ggpairs(Data, columns = c(1:3,5:8,10:13), 
        title = "Relación entre las variables independientes y 'medv'",
        upper = list(continuous = wrap("cor", size = 2.5)),
        progress = FALSE)
gpairs
```

Los resultados de correlación proporcionan información importante respecto a posible explicación de la varianza de la variable de respuesta (esto es, en función de las variables independientes) y en cuanto a la existencia o no de colinealidad (relación entre las variables independientes). Para la interpretación de los resultados de coeficiente de correlación nos enfocamos en aquellos que están alrededor de 0.7 o por encima. La función ggpairs (de la librería GGally) con la cual se construyó el anterior gráfico presenta los resultados de correlación calculados con el método "Pearson".

Frente a los resultado se resalta lo siguiente:

-   Los valores absolutos más altos de correlación se encuentran alrededor de 0.7.

-   La variable de respuesta presenta la mayor correlación con la variable *lstat* la cual está en -0.738, siendo esta negativa, es decir a medida que aumenta el valor de *lstat* disminuye el valor de *medv.*

-   Un valor de correlación frente a la variable de respuesta que está muy cerca a 0.7 es *rm,* esta correlación se posiciona en 0.695, esto es, una relación directamente proporcional.

-   Las demás variables independientes no tienen un nivel de correlación importante, pues los valores absolutos de correlación se encuentran alrededor de 0.5 y 0.2.

-   Al analizar las variables independientes se evidencia que *nox* tiene un nivel de correlación importante con tres variables: *age*, *dis e indus* de las cuales se tiene coeficientes de correlación de 0.731, -0.769 y 0.764, respectivamente, entre *dis* y *age* se observa una relación -0.748.

-   Otra variable con unos valores importantes de relación es *indus*, adicional a la ya mencionada con *nox*, se observan valores de R de 0.645 con la variable *age*, -0.708 con la variable *dis* y 0.721 con la variable *tax*.

Si bien las conclusiones anteriormente expuestas requieren pruebas de hipótesis que permitan concluir que estos resultados son válidos y aplicables a toda la población, el gráfico de correlación presentado incluye información implícita de dicha prueba lo que permite soportarlas. No obstante en una sección posterior se lo valores que permitan concluir si existe o no relación entre variables.

### 1.2. Medición de la varianza y relación entre variables

#### 1.2.1 Matriz de covarianza

La matriz de covarianza valida los resultados de correlación anteriormente presentados en cuanto a la dirección de la relación existente entre par de variables, esto es, si es inversamente proporcional (valores negativos) o directamente proporcional (valores positivos). Las "cantidad"" de varianza (diagonal) y de covarianza no pueden interpretarse teniendo en cuenta las magnitudes de la matriz por si solos, se debe acompañar con los valores de la medias.

```{r echo=FALSE, message=FALSE, warning=FALSE}
mcov <- data.frame(round(cov(Data[,-c(4,9)]),3))
knitr :: kable(mcov)
```

#### 1.2.2. Matriz de correlaciones

Los resultados presentados en la matriz a continuación son los mismos observados en el gráfico de pares (se usa el mismo método), sin embargo, los valores se extraen en una tabla para posteriormente calcular los p-valor.

```{r echo=FALSE, message=FALSE, warning=FALSE}
mcor <- data.frame(round(cor(Data[,-c(4,9)]),2))
knitr :: kable(mcor)
```

Los datos de la siguiente matriz son los p-valor que me permiten, frente a la definición del nivel de significancia, concluir respecto a la existencia o no de correlación, es decir, si el coeficiente de correlación es cero o diferente de cero:

Ho: R = 0

Ha: R != 0

Los p-valores calculados con un nivel de confianza de 95% son:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Función para crear matriz con los p-valor de los índices de correlación
cor.mtest <- function(M, ...) {
  mat <- as.matrix(M)
  n <- ncol(M)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(M[, i], M[, j], method = "pearson", conf.level = 0.95)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(M)
  return(p.mat)
}
PMat<-cor.mtest(cor(Data[,-c(4,9)]))
knitr :: kable(PMat)
```

En este orden de ideas, ninguno de los p-valor observados son mayores a 0.05, por lo que en todos los casos se rechaza la hipótesis que establece que la correlación es 0.

#### 1.2.3. Evaluación de multicolinealidad

Con base en el anterior resultado y los coeficientes se puede concluir respecto al "nivel de colinealidad" por pares de variables independientes. En la siguiente gráfica, para círculos grandes, tendientes a color azul existe una correlación positiva mientras que para círculos grandes, tendientes a color rojo existe una correlación negativa.

```{r echo=FALSE, message=FALSE, warning=FALSE}
corrplot(cor(Data[,-c(4,9)]), type="lower", tl.col="black", tl.srt=45, 
         method="circle", addCoef.col = "black", number.cex = 0.5, p.mat=PMat
         ,sig.level = 0.05, insig = "blank", diag=FALSE)
```

Los resultados obtenidos en esta primera parte permiten identificar que variables pueden tener más impacto en modelos de regresión, cuales podrían excluirse de los modelos a plantearse y adicionalmente reconocer que cierta combinación de variables (que presentan colinealidad) podrían afectar los modelos resultantes en cuanto a la precisión en las predicciones.
